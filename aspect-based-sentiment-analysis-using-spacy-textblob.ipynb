{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis to be aspect-based, or otherwise called topic-based\n",
    "\n",
    "# conda install -c conda-forge spacy\n",
    "\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get started by importing spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = \"We had some amazing food yesterday. But the next day was very boring.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We : nsubj had VERB PRON []\n",
      "had : ROOT had VERB VERB [We, food, yesterday, .]\n",
      "some : det food NOUN DET []\n",
      "amazing : amod food NOUN ADJ []\n",
      "food : dobj had VERB NOUN [some, amazing]\n",
      "yesterday : npadvmod had VERB NOUN []\n",
      ". : punct had VERB PUNCT []\n",
      "But : cc was AUX CCONJ []\n",
      "the : det day NOUN DET []\n",
      "next : amod day NOUN ADJ []\n",
      "day : nsubj was AUX NOUN [the, next]\n",
      "was : ROOT was AUX AUX [But, day, boring, .]\n",
      "very : advmod boring ADJ ADV []\n",
      "boring : acomp was AUX ADJ [very]\n",
      ". : punct was AUX PUNCT []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(st)\n",
    "for token in doc:\n",
    "    print(token.text, ':', token.dep_, token.head.text, token.head.pos_, token.pos_, [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'The food we had yesterday was delicious',\n",
    "  'My time in Italy was very enjoyable',\n",
    "  'I found the meal to be tasty',\n",
    "  'The internet was slow.',\n",
    "  'Our experience was suboptimal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first goal is to split our sentences in a way so that we have the target aspects (e.g. food)\n",
    "# and their sentiment descriptions (e.g. delicious)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det food NOUN DET []\n",
      "food nsubj was AUX NOUN [The, had]\n",
      "we nsubj had VERB PRON []\n",
      "had relcl food NOUN VERB [we, yesterday]\n",
      "yesterday npadvmod had VERB NOUN []\n",
      "was ROOT was AUX AUX [food, delicious]\n",
      "delicious acomp was AUX ADJ []\n",
      "My poss time NOUN PRON []\n",
      "time nsubj was AUX NOUN [My, in]\n",
      "in prep time NOUN ADP [Italy]\n",
      "Italy pobj in ADP PROPN []\n",
      "was ROOT was AUX AUX [time, enjoyable]\n",
      "very advmod enjoyable ADJ ADV []\n",
      "enjoyable acomp was AUX ADJ [very]\n",
      "I nsubj found VERB PRON []\n",
      "found ROOT found VERB VERB [I, be]\n",
      "the det meal NOUN DET []\n",
      "meal nsubj be VERB NOUN [the]\n",
      "to aux be VERB PART []\n",
      "be ccomp found VERB VERB [meal, to, tasty]\n",
      "tasty acomp be VERB ADJ []\n",
      "The det internet NOUN DET []\n",
      "internet nsubj was AUX NOUN [The]\n",
      "was ROOT was AUX AUX [internet, slow, .]\n",
      "slow acomp was AUX ADJ []\n",
      ". punct was AUX PUNCT []\n",
      "Our poss experience NOUN PRON []\n",
      "experience nsubj was AUX NOUN [Our]\n",
      "was ROOT was AUX AUX [experience, suboptimal]\n",
      "suboptimal acomp was AUX ADJ []\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "      token.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NOUN DET\n",
      "food AUX NOUN\n",
      "we VERB PRON\n",
      "had NOUN VERB\n",
      "yesterday VERB NOUN\n",
      "was AUX AUX\n",
      "delicious AUX ADJ\n",
      "My NOUN PRON\n",
      "time AUX NOUN\n",
      "in NOUN ADP\n",
      "Italy ADP PROPN\n",
      "was AUX AUX\n",
      "very ADJ ADV\n",
      "enjoyable AUX ADJ\n",
      "I VERB PRON\n",
      "found VERB VERB\n",
      "the NOUN DET\n",
      "meal VERB NOUN\n",
      "to VERB PART\n",
      "be VERB VERB\n",
      "tasty VERB ADJ\n",
      "The NOUN DET\n",
      "internet AUX NOUN\n",
      "was AUX AUX\n",
      "slow AUX ADJ\n",
      ". AUX PUNCT\n",
      "Our NOUN PRON\n",
      "experience AUX NOUN\n",
      "was AUX AUX\n",
      "suboptimal AUX ADJ\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  for token in doc:\n",
    "    print(token.text,token.head.pos_,  token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token inside our sentences, we can see the dependency thanks to spacy’s dependency parsing and the POS (Part-Of-Speech) tags. We’re also paying attention to the child tokens, so that we’re able to pick up intensifiers such as “very”, “quite”, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "My time in Italy was very enjoyable\n",
      "enjoyable\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "The internet was slow.\n",
      "slow\n",
      "Our experience was suboptimal\n",
      "suboptimal\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    descriptive_term = ''\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            descriptive_term = token\n",
    "    print(sentence)\n",
    "    print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': 'delicious'}, {'aspect': 'time', 'description': 'very enjoyable'}, {'aspect': 'meal', 'description': 'tasty'}, {'aspect': 'internet', 'description': 'slow'}, {'aspect': 'experience', 'description': 'suboptimal'}]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    descriptive_term = ''\n",
    "    target = ''\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "            target = token.text\n",
    "        if token.pos_ == 'ADJ':\n",
    "            prepend = ''\n",
    "            for child in token.children:\n",
    "                if child.pos_ != 'ADV':\n",
    "                    continue\n",
    "                prepend += child.text + ' '\n",
    "            descriptive_term = prepend + token.text\n",
    "    aspects.append({'aspect': target, 'description': descriptive_term})\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we successfully extracted the aspects and descriptions, it’s time to classify them as positive or negative. The goal here is to help the computer understand that tasty food is positive, while slow internet is negative. Computers don’t understand English, so we will need to try a few things before we have a working solution.\n",
    "We will start off by using the default TextBlob sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "    aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect: food\n",
      "description: delicious\n",
      "sentiment: Sentiment(polarity=1.0, subjectivity=1.0)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "aspect: time\n",
      "description: very enjoyable\n",
      "sentiment: Sentiment(polarity=0.65, subjectivity=0.78)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "aspect: meal\n",
      "description: tasty\n",
      "sentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "aspect: internet\n",
      "description: slow\n",
      "sentiment: Sentiment(polarity=-0.30000000000000004, subjectivity=0.39999999999999997)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "aspect: experience\n",
      "description: suboptimal\n",
      "sentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "for i in aspects:\n",
    "    print('aspect:',i['aspect'])\n",
    "    print('description:',i['description'])\n",
    "    print('sentiment:',i['sentiment'])\n",
    "    print('+'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicious food.\n",
      "positive\n",
      "Very Slow internet.\n",
      "negative\n",
      "Suboptimal experience.\n",
      "negative\n",
      "Enjoyable food.\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "# We train the NaivesBayesClassifier\n",
    "train = [\n",
    "  ('Slow internet.', 'negative'),\n",
    "  ('Delicious food', 'positive'),\n",
    "  ('Suboptimal experience', 'negative'),\n",
    "  ('Very enjoyable time', 'positive'),\n",
    "  ('delicious food.', 'neg')\n",
    "]\n",
    "cl = NaiveBayesClassifier(train)\n",
    "# And then we try to classify some sample sentences.\n",
    "blob = TextBlob(\"Delicious food. Very Slow internet. Suboptimal experience. Enjoyable food.\", classifier=cl)\n",
    "for s in blob.sentences:\n",
    "    print(s)\n",
    "    print(s.classify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicious ice.\n",
      "pos\n",
      "Very Slow internet.\n",
      "negative\n",
      "Suboptimal experience.\n",
      "negative\n",
      "Enjoyable food.\n",
      "pos\n",
      "not good ride.\n",
      "negative\n",
      "good ride.\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "# We train the NaivesBayesClassifier\n",
    "train = [\n",
    "  ('Slow internet.', 'negative'),\n",
    "  ('Delicious food', 'pos'),\n",
    "  ('Suboptimal experience', 'negative'),\n",
    "  ('Very enjoyable time', 'positive'),\n",
    "  ('enjoyable ride', 'positive'),\n",
    "  ('not enjoyable ride', 'negative'),\n",
    "    ('good gift', 'positive'),\n",
    "  ('delicious food.', 'neg')\n",
    "]\n",
    "cl = NaiveBayesClassifier(train)\n",
    "# And then we try to classify some sample sentences.\n",
    "blob = TextBlob(\"Delicious ice. Very Slow internet. Suboptimal experience. Enjoyable food. not good ride. good ride.\", classifier=cl)\n",
    "for s in blob.sentences:\n",
    "    print(s)\n",
    "    print(s.classify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
